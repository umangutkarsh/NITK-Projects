{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7f85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd52adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some constants/hyperparameters\n",
    "BUFFER_SIZE = 70_000 # for reshuffling\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9580b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b060bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac73d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c10f6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc2b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44844465",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9900f59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45a8e9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257355e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    \n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b477e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b346407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "291dafee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc959deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b226fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce99314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=6000>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12f455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=10000>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14c730cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9c47927",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "validation_data = train_and_validation_data.take(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de9948e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9cc57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the numpy arrays from the validation data for the calculation of the Confusion Matrix\n",
    "for images, labels in validation_data:\n",
    "    images_val = images.numpy()\n",
    "    labels_val = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2faa407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec3d2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=50, kernel_size=5, activation='relu', input_shape=[28,28,1]))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=50, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5415e7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "___________________________________________________________________________\n",
      " Layer (type)                    Output Shape                  Param #     \n",
      "===========================================================================\n",
      " conv2d (Conv2D)                 (None, 24, 24, 50)            1300        \n",
      "                                                                           \n",
      " max_pooling2d (MaxPooling2D)    (None, 12, 12, 50)            0           \n",
      "                                                                           \n",
      " conv2d_1 (Conv2D)               (None, 10, 10, 50)            22550       \n",
      "                                                                           \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 50)              0           \n",
      "                                                                           \n",
      " flatten (Flatten)               (None, 1250)                  0           \n",
      "                                                                           \n",
      " dense (Dense)                   (None, 10)                    12510       \n",
      "                                                                           \n",
      "===========================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a107a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ec53c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e124bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be55ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4893a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"Logs\\\\fit\\\\\" + \"run-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92752716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23803ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    \n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside the notebook.\n",
    "    plt.close(figure)\n",
    "    \n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a15ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a file writer variable for logging purposes\n",
    "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    test_pred_raw = model.predict(images_val)\n",
    "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    figure = plot_confusion_matrix(cm, class_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5e2d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552329a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f66e6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "544c8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           min_delta=10, \n",
    "                           patience=6, \n",
    "                           restore_best_weights=True, \n",
    "                           mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52e32725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 [==============================] - 37s 85ms/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.0070 - val_accuracy: 0.9982\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 36s 83ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0064 - val_accuracy: 0.9983\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 37s 85ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0082 - val_accuracy: 0.9970\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 37s 85ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0070 - val_accuracy: 0.9973\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 35s 81ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0078 - val_accuracy: 0.9980\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 36s 83ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.0090 - val_accuracy: 0.9963\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 37s 86ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.0045 - val_accuracy: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19307040a30>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, \n",
    "          validation_data=validation_data, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          verbose=1, \n",
    "          callbacks=[tensorboard_callback, cm_callback, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e6d54f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0312 - accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a7a9372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0312. Test accuracy: 99.13%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {:.4f}. Test accuracy: {:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67ca8c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b0068e000c9b94a5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b0068e000c9b94a5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs/fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc2365d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f69a1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test_data into 2 arrays, containing the images and the corresponding labels\n",
    "for images, labels in test_data.take(1):\n",
    "    images_test = images.numpy()\n",
    "    labels_test = labels.numpy()\n",
    "\n",
    "# Reshape the images into 28x28 form, suitable for matplotlib (original dimensions: 28x28x1)\n",
    "images_plot = np.reshape(images_test, (10000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59bfc231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAAB7CAYAAACy7jQ7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFAklEQVR4nO2dSyh9axjG93a/pIgMXMploCQlxEghlwEilwxdkjJVBsrMhIGBUEQZUC4hYiIRGYkBA7cwEUZyF6F9BucMzvOuv6V97L0t531+s197r7W+7fHtd69vfetbdofDYSP68PrpBpCfgcErhcErhcErhcErhcErxcfsRbvdznO9X4zD4bB/9hp7vFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFIYvFJMp15ZHS8v/L9tbm4GLysrAy8qKgK32z+dmWSz2Ww2eZfR1dUV+MzMDHh/fz/40dGR6f5/EvZ4pTB4pTB4pdjN7pa1+vTqiYkJ8Orq6h9qyd+8vr6CNzU1gY+Pj3uyOZxeTYwweKUweKVYusZHRESAz83NgWdlZYF7e3uDy8/29PQEPjQ0ZHr8oKAg8MbGRnAfHxwGkeMCLy8v4Onp6eCHh4emx/8urPHEAINXCoNXiqVrfE5ODvja2prp+3d3d8Hb2trAV1ZWXNOwf6isrASX5+m+vr7gg4OD4C0tLS5tj4Q1nhhg8Eph8EqxVI2XNfHs7Aw8KioK/OPjA1xeb//qN8F3iYuLAz84OAD38/MDv7y8BI+Pjwd/f393XeNsrPHkDzB4pTB4pVhqzl1DQwO4rOkSef3b3TVd0traCi5ruiQyMhL8qzl/7oQ9XikMXikMXimWqvHy+vdXyHn14eHh4NfX199qj/yNkZaWBl5bW+vU/kZGRsDf3t7+W8NcAHu8Uhi8Uhi8Uiw1Vh8bGwu+uroKnpCQYLq9vN49NjYGfnd3B56SkgIu5+VnZmaCx8TEmB5fMjk5Cd7R0QF+enrq1P6chWP1xACDVwqDV4qlarykrq4OvLe3Fzw4ONh0ezmWL6/fOztuIHl4eADv6uoC7+npMW2Pu2GNJwYYvFIsNWQrGR0dBQ8MDATv6+sz3d7f39/VTQLkLVjd3d3gsrRYCfZ4pTB4pTB4pVj6dC4xMRF8Z2cHPCQkBPzm5gZ8aWkJPDU11dS/izyda29vd+n+nYWnc8QAg1cKg1eKpWq8XMpkY2MDPDs7G/z+/h5c3kK1tbUFHhAQYOpJSUngJSUl4HI6tRwnkH/L8vJy8MXFRZsnYY0nBhi8Uhi8UixV4+Uy43J5M0leXh74+vq6y9v0b+rr68GHh4dN3z81NQUul0t7fn52TcM+gTWeGGDwSmHwSrHU9Xg5Nv8VJycnbmrJn5mengaX4wZyenZNTQ347Oys6f48CXu8Uhi8Uhi8UixV463O4+Mj+FdLtUhyc3PBWeOJx2HwSmHwSvnVNT46Ohr84uLCrccLDQ0Fd/dYuzthj1cKg1cKg1fKr67x8np9fn4++Hcf75WRkQHe2dkJXlBQYLq9XKZ8YGDgW+1xJezxSmHwSmHwSrHUnDv5aJKFhQXwwsJC0+2Xl5fB9/f3Td+/t7cHXlVVBV5cXAwul1CVyOPJx5MdHx+bbu9qOOeOGGDwSmHwSrFUjZfIx3ttb2+Dh4WFebA1Ntvt7S24vDevoqICXD5G3NOwxhMDDF4plv6ql8jVo+XTmEtLS8GTk5Od2r+cCjU/Pw++ubkJfn5+7tT+PQ2/6okBBq8UBq+UX1XjiXOwxhMDDF4pDF4pDF4pDF4pDF4pDF4pDF4pDF4pDF4pDF4ppmP15P8Le7xSGLxSGLxSGLxSGLxSGLxS/gI9mGzUI75kPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    }
   ],
   "source": [
    "i = 7787\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c1882494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the model's predictions (logits)\n",
    "predictions = model.predict(images_test[i-1:i])\n",
    "\n",
    "# Convert those predictions into probabilities (recall that we incorporated the softmaxt activation into the loss function)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()\n",
    "# Convert the probabilities into percentages\n",
    "probabilities = probabilities*100\n",
    "\n",
    "\n",
    "# Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca6918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
